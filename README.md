# MATH-475_Final_Project

## Summary

1. Introduction <br/><br/>
     The dataset I have decided to work with contains the clinical data of 1025 people. The clinical data is split into 13 features, excluding the target variable, and these features are: age, sex, chest pain type (cp) split into 4 values, resting blood pressure (trestbps), serum cholestoral in mg/dl (chol), fasting blood sugar > 120 mg/dl (fbs), resting electrocardiographic results (restecg) split into values of 0,1, and 2, maximum heart rate achieved (thalach), exercise induced angina (exang)  ST depression induced by exercise relative to rest (oldpeak), the slope of the peak exercise ST segment (slope), number of major vessels (0-3) colored by flourosopy (ca), and thal: 0 for normal, 1 for fixed defect, and 2 for reversable defect. These features will be analyzed for use in predicting the target variable, that is whether the person has no heart disease, indicated by a 0, or has heart disease, indicated by a 1. I found this dataset to be the most appropriate, as it contained the most data points, as well as significant features that could clearly effect the presence of heart disease in a person.

2. Data Exploration and Preprocessing <br/><br/>
     For data exploration, I first displayed a few entries in my data set, viewed the shape of my data, and looked at the basic information of my features. After doing so, I proceeded to view some of the statistics associated with the previously mentioned features and visualized their distribution. It was noted from the pair plot that people with a high maximum heart rate tended to be more prone to heart disease. Then, using box plots, outliers were found. There were only a few, such as a resting blood pressure that was almost at the maximum of 200 mm Hg, and a high maximum value of 564 mg/dl for cholesterol. Since they were minimal, a decision was made to leave the outliers. A correlation heatmap was then used to get further my understanding of which features correlated with the target variable, such as chest pain type and maximum heart rate. As for preprocessing, I first removed duplicate entries. Then I proceeded to one-hot encode my categorical variables to move towards a form that is easier for my model to interpret. Finally, I decided to scale my numerical features, in hopes that each category would be treated more fairly when training.

3. Model Selection and Training<br/>
     Initially, I had chosen to have only one model, and that was a Logistic Regression model. I chose this model because logistic regression models output probabilities between 0 and 1, which is perfect for binary classification tasks such as this one. As for training this model, I ended up going the standard route. I defined X and y, dropping the target from X and setting it to y. I then split my dataset into training and testing sets, 80% and 20% respectively, and fit the model of 10,000 iterations. The accuracy score was okay, coming out to 80%, and I tried improving this score through hyperparameter tuning, but all my attempts made the model perform worse. This included a grid search to find the best parameters for the model, as well as using PCA to find the best features. So, my path led me back to the barebones model. Since I could not improve the performance through hyperparameter tuning, I decided to train another model, and that was a Neural Network. I hoped that by choosing this model, it would be able to more accurately find complex patterns in the data through the non-linear relationships that may exist. I started the same way as with the first model, defining X and y, dropping the target from X, and setting it to y. I then split my dataset into training and testing sets, 80% and 20% respectively. The model had three hidden layers, all using ReLU activation, and a final output layer for binary classification using sigmoid activation. To avoid overfitting, I used dropout layers and L2 regularization, and batch normalization was used to help in the training process. I went with Adam for the optimizer, because, after research, it was supposed to help with converging by adapting the learning rate for each parameter. The model was trained for 100 epochs. This final model was an improvement on a previous attempt that had two layers and used the sgd optimizer. The accuracy for the final Neural Network model came out to 85%.

4. Performance Evaluation<br/>
     The Neural Network achieved an accuracy of 85%, while the Logistic Regression model achieved 80%. The accuracy indicates the proportion of correct predictions, positive or negative, so the models performed well, but there is possibly still room for improvement. The recall score was higher for both categories in the Neural Network as well, being 81% for category 0 and 89% for category 1. For the Logistic Regression model, the recall was 75% for category 0 and the recall for category 1 was 86%. As mentioned previously, recall measures the true positive rate, and since diagnosing a severe medical problem such as heart disease, if the patient does indeed have it, is critical. I would say this makes sense to me, as the Neural Network may be able to notice non-linear patterns in the data, improving its effectiveness. But I will say, that the Logistic Regression model is definitely more consistent, as it will always produce the same accuracy score. With the Neural Network, it bounces around because different runs of the 100 epochs produce different results, some of these being worse than the Logistic Regression model. For further improvement on the accuracy scores, I may try to go with another model that uses Gradient Boosting, like XGBoost, to see if that would perform better.

5. Challenges to Overcome<br/>
     One major challenge I faced initially had to do with my Logistic Regression model. In the beginning, I tried many avenues of hyperparameter tuning as mentioned previously, such as using gridsearch to search for the best parameters to using for my model as well as PCA, but all of my efforts resulted in worse performance for my model (some of the later attempts were removed for conciseness). So after pondering for a better solution, I went with what seemed to be the simplest one, and that was to do what was performing the best. So I went with the baseline model to overcome this challenge, as well as making an attempt at a different model, the Neural Network. Initially, I did not set out to make multiple models, but since my attempts at improvement for the Logistic Regression model were unsuccessful, I went with this avenue. Another challenge was that initially, my Neural Network was performing way below the Logistic Regression model in accuracy, as I only had two layers, and I tried using the optimizer sgd. After further study, to help my model with the complexity, I decided to switch my optimizer to Adam, as I found that to be the best optimizer for binary classification tasks, as well as add another layer to the Network. This, along with L2 regularization and batch normalization helped this version perform better than the last. Despite these efforts to increase the accuracy, I believe training instability is an issue with the Neural Network model. Different runs showed substantial variability in the accuracy and recall scores. I believe future work could benefit from exploring a model that uses Gradient Boost, like XGBoost, as mentioned previously (see Comparison Section in Model Selection and Training). I also believe that it could benefit from trying to expand the data I am working with, since after removing duplicates, my data may limit my model's ability to generalize well.


## Note
This is just a summary. All visuals and code can be seen via the jupyter notebook in the repository.
